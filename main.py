import streamlit as st
from openai import OpenAI
from utils import render_with_latex
from prompts import (
    for_system_prompt_with_reference,
    for_system_prompt_without_reference,
    system_prompt,
    COMMON_INSTRUCTIONS,
    feedback_analysis_prompt,
    INTENT_CLASSIFICATION_PROMPT
)
from sidebar import render_sidebar
from database import Database
import os
import json
import PyPDF2

def read_pdf_content(pdf_path):
    """PDF íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì„œ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    content = ""
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            content += page.extract_text()
    return content

def classify_user_intent(user_input, client):
    """ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    try:
        prompt = INTENT_CLASSIFICATION_PROMPT.format(user_input=user_input)
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=200
        )
        
        content = response.choices[0].message.content
        content = content.replace("```json", "").replace("```", "").strip()
        content = " ".join(line.strip() for line in content.splitlines())
        result = json.loads(content)
        
        return result
        
    except Exception as e:
        st.error(f"ì˜ë„ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {"intent": "Learning", "confidence": 0.5, "reason": "ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ê°’"}

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
ADDIE_PDF_PATH = "ADDIE_Model_All_Stages_Detailed_Concepts_with_References.pdf"

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”

# ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìœ¼ë©´ Falseë¡œ ì´ˆê¸°í™”
if "system_prompt_created" not in st.session_state:
    st.session_state.system_prompt_created = False

# í˜„ì¬ ëŒ€í™” ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ Noneë¡œ ì´ˆê¸°í™”
if "current_conversation_id" not in st.session_state:
    st.session_state.current_conversation_id = None

# API í‚¤ ìœ íš¨ì„± ê²€ì¦ ìƒíƒœê°€ ì—†ìœ¼ë©´ Falseë¡œ ì´ˆê¸°í™”
if "api_key_valid" not in st.session_state:
    st.session_state.api_key_valid = False

# ëŒ€í™” ëª¨ë“œê°€ ì—†ìœ¼ë©´ Noneë¡œ ì´ˆê¸°í™”
if "conversation_mode" not in st.session_state:
    st.session_state.conversation_mode = None

# íˆìŠ¤í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì™”ëŠ”ì§€ í‘œì‹œí•˜ëŠ” í”Œë˜ê·¸
if "history_loaded" not in st.session_state:
    st.session_state.history_loaded = False

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
db = Database()

# ADDIE ë¬¸ì„œê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ìœ¼ë©´ ì €ì¥ ì‹œë„
if not db.get_addie_document():
    try:
        if os.path.exists(ADDIE_PDF_PATH):
            addie_content = read_pdf_content(ADDIE_PDF_PATH)
            db.save_addie_document(addie_content)
    except Exception as e:
        st.info("ADDIE ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLMì˜ ì¶”ë¡ ì— ê¸°ë°˜í•˜ì—¬ ì§„í–‰í•©ë‹ˆë‹¤.")

# Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="Dusan Baek", page_icon="ğŸ§‘â€ğŸ«")
st.title("ğŸ§‘â€ğŸ« AI Tutor")

# ì‚¬ì´ë“œë°” ë Œë”ë§
render_sidebar()

# API í‚¤ ìœ íš¨ì„± ê²€ì¦ ìƒíƒœ í™•ì¸
if not st.session_state.api_key_valid:
    st.warning("OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ìœ íš¨í•œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()  # ì—¬ê¸°ì„œ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ì—¬ ì±„íŒ… ê¸°ëŠ¥ ì œí•œ

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
api_key = st.session_state.get("openai_api_key", st.secrets.get("openai", {}).get("api_key", ""))
client = OpenAI(api_key=api_key)

# íˆìŠ¤í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¨ ê²½ìš° ë‹µë³€ ìƒì„± ë¡œì§ì„ ê±´ë„ˆëœ€
if st.session_state.get("history_loaded", False):
    st.session_state.history_loaded = False  # í•œ ë²ˆë§Œ ê±´ë„ˆëœ€
    # ë©”ì‹œì§€ ë Œë”ë§Œ í•˜ê³ , ë‹µë³€ ìƒì„±/appendëŠ” í•˜ì§€ ì•ŠìŒ

# ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥ (ì²« ë²ˆì§¸ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
if st.session_state.messages and st.session_state.system_prompt_created:
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "system":
                if msg["content"] == "REFRESH":
                    continue  # REFRESH ë©”ì‹œì§€ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ
                # êµìœ¡ ëª¨ë“œì¸ ê²½ìš°ì—ë§Œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í‘œì‹œ
                if st.session_state.conversation_mode == "educational":
                    with st.expander("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³´ê¸°"):
                        st.markdown(render_with_latex(msg["content"]))
                # ì¼ë°˜ ëŒ€í™” ëª¨ë“œì—ì„œëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ¨ê¹€
            elif msg["role"] == "assistant":
                st.markdown(render_with_latex(msg["content"]))
            else:
                st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

def analyze_feedback(current_context, user_feedback):
    """ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ í•™ìŠµ ìƒíƒœë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    try:
        # í”¼ë“œë°± ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = feedback_analysis_prompt.format(
            current_context=current_context,
            user_feedback=user_feedback
        )
        
        # í”¼ë“œë°± ë¶„ì„ ìš”ì²­
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1000
        )
        
        # JSON ì‘ë‹µ íŒŒì‹±
        content = response.choices[0].message.content
        content = content.replace("```json", "").replace("```", "").strip()
        content = " ".join(line.strip() for line in content.splitlines())
        result = json.loads(content)
        
        return result
        
    except Exception as e:
        st.error(f"í”¼ë“œë°± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {"status": "ì§„í–‰", "reason": "ì˜¤ë¥˜ ë°œìƒ", "feedback_type": "ê¸°íƒ€"}

if user_input:
    # ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ ê²½ìš° ì˜ë„ ë¶„ë¥˜ ë° ì²˜ë¦¬
    if not st.session_state.messages:
        # ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ìƒì„±
        if not st.session_state.current_conversation_id:
            conversation_title = user_input[:50] + "..." if len(user_input) > 50 else user_input
            st.session_state.current_conversation_id = db.create_conversation(conversation_title)
        
        # ì˜ë„ ë¶„ë¥˜
        with st.spinner("ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘..."):
            intent_result = classify_user_intent(user_input, client)
            
            # Learningì¸ ê²½ìš°ì—ë§Œ êµìœ¡ ëª¨ë“œë¡œ ì„¤ì •
            if intent_result["intent"] == "Learning":
                st.session_state.conversation_mode = "educational"
                st.info(f"ğŸ“ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤. (ì˜ë„: {intent_result['intent']})")
                
                # êµìœ¡ ëª¨ë“œ: ADDIE í”„ë ˆì„ì›Œí¬ ìƒì„±
                with st.spinner("êµìˆ˜ ì„¤ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ..."):
                    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ADDIE ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                    addie_reference_content = db.get_addie_document()
                    
                    # í”„ë¡¬í”„íŠ¸ ìƒì„±
                    if addie_reference_content:
                        prompt = for_system_prompt_with_reference.format(
                            user_input=user_input,
                            addie_reference_content=addie_reference_content,
                            common_instructions=COMMON_INSTRUCTIONS
                        )
                        
                    else:
                        prompt = for_system_prompt_without_reference.format(
                            user_input=user_input,
                            common_instructions=COMMON_INSTRUCTIONS
                        )
                    
                    
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}],
                        temperature=0.7,
                        max_tokens=2000
                    )
                    
                    try:
                        # JSON ì‘ë‹µ íŒŒì‹±
                        content = response.choices[0].message.content
                        
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í‘œì‹œ ì œê±°
                        content = content.replace("```json", "").replace("```", "").strip()
                        
                        # JSON ì •ê·œí™” (ì—¬ëŸ¬ ì¤„ì„ í•œ ì¤„ë¡œ)
                        content = " ".join(line.strip() for line in content.splitlines())
                        
                        result = json.loads(content)
                        
                        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                        system_prompt_content = system_prompt.format(
                            analysis_content=result["analysis_content"],
                            design_content=result["design_content"]
                        )
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        if st.session_state.current_conversation_id:
                            db.save_message(st.session_state.current_conversation_id, "system", system_prompt_content)
                        
                        st.session_state.system_prompt_created = True
                        st.session_state.messages.append({"role": "system", "content": system_prompt_content})
                        
                    except (json.JSONDecodeError, KeyError) as e:
                        st.error(f"í”„ë ˆì„ì›Œí¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                        st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                        
                        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                        st.session_state.messages = []
                        st.session_state.system_prompt_created = False
                        st.session_state.current_conversation_id = None
                        st.session_state.conversation_mode = None
                        
                        # ì¬ì‹œë„ ë²„íŠ¼
                        if st.button("ë‹¤ì‹œ ì‹œë„", key="retry_button"):
                            st.rerun()
                        
                        st.stop()  # ì—¬ê¸°ì„œ ì‹¤í–‰ ì¤‘ë‹¨
            else:
                # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
                st.session_state.conversation_mode = "casual"
                st.info(f"ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œì…ë‹ˆë‹¤. (ì˜ë„: {intent_result['intent']})")
                
                # ê°„ë‹¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                casual_system_prompt = """
                ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                - ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
                - í•„ìš”í•œ ê²½ìš° ë§ˆí¬ë‹¤ìš´ê³¼ LaTeXë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
                - ì‚¬ìš©ìê°€ ë§Œì¡±í•  ìˆ˜ ìˆë„ë¡ ë„ì›€ì„ ì£¼ì„¸ìš”
                """
                st.session_state.messages.append({"role": "system", "content": casual_system_prompt})
                st.session_state.system_prompt_created = True

            # ì‚¬ìš©ìì˜ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": user_input})
            db.save_message(st.session_state.current_conversation_id, "user", user_input)

            # AI ì‘ë‹µ ì¶œë ¥ ì˜ì—­
            with st.chat_message("assistant"):
                stream_placeholder = st.empty()
                full_response = ""

                try:
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=st.session_state.messages,
                        stream=True
                    )

                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
                    for chunk in response:
                        if chunk.choices and chunk.choices[0].delta.content:
                            content = chunk.choices[0].delta.content
                            full_response += content
                            stream_placeholder.markdown(render_with_latex(full_response + "â–Œ"))

                    # ìŠ¤íŠ¸ë¦¬ë° ëë‚œ í›„ ìˆ˜ì‹ í¬í•¨í•´ì„œ ë‹¤ì‹œ ë Œë”ë§
                    stream_placeholder.empty()
                    st.markdown(render_with_latex(full_response))

                    # ì‘ë‹µ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
                    if not (st.session_state.messages and
                            st.session_state.messages[-1]["role"] == "assistant" and
                            st.session_state.messages[-1]["content"] == full_response):
                        st.session_state.messages.append(
                            {"role": "assistant", "content": full_response}
                        )
                        db.save_message(st.session_state.current_conversation_id, "assistant", full_response)
                    
                    # í™”ë©´ ê°±ì‹ ì„ ìœ„í•œ rerun
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    st.stop()
    else:

        with st.chat_message("user"):
            st.markdown(user_input)
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": user_input})
            db.save_message(st.session_state.current_conversation_id, "user", user_input)
        
        # êµìœ¡ ëª¨ë“œì¸ ê²½ìš°ì—ë§Œ í”¼ë“œë°± ë¶„ì„ ìˆ˜í–‰
        if st.session_state.conversation_mode == "educational":
            # ë‘ ë²ˆì§¸ ë©”ì‹œì§€ë¶€í„°ëŠ” í”¼ë“œë°± ë¶„ì„
            # ì´ì „ ë©”ì‹œì§€ê°€ 3ê°œ ë¯¸ë§Œì¸ ê²½ìš°ëŠ” ìˆëŠ” ë§Œí¼ë§Œ ì‚¬ìš©
            context_messages = st.session_state.messages[-3:] if len(st.session_state.messages) >= 3 else st.session_state.messages
            current_context = "\n".join([msg["content"] for msg in context_messages])
            
            feedback_analysis = analyze_feedback(current_context, user_input)
            print(f"[FEEDBACK ANALYSIS] {feedback_analysis}")  # í”¼ë“œë°± ë¶„ì„ ê²°ê³¼ ë¡œê·¸
            # í”¼ë“œë°±ì´ "í‰ê°€" ìƒíƒœì¸ ê²½ìš° ìƒˆë¡œìš´ ë¶„ì„ê³¼ ì„¤ê³„ ë°˜ì˜
            if feedback_analysis["status"] == "evaluation" and "suggested_adjustment" in feedback_analysis:
                print("[FEEDBACK] evaluation detected, updating system prompt...")  # ë¶„ë¥˜ ë¡œê·¸
                # ê¸°ì¡´ system ë©”ì‹œì§€ ì°¾ê¸° (ê°€ì¥ ì²« ë²ˆì§¸ system ë©”ì‹œì§€)
                for idx, msg in enumerate(st.session_state.messages):
                    if msg["role"] == "system" and msg["content"] != "REFRESH":
                        system_idx = idx
                        break
                else:
                    system_idx = None
                
                if system_idx is not None:
                    # ê¸°ì¡´ system promptì— í”¼ë“œë°± ë‚´ìš©ì„ ì¤„ ë‹¨ìœ„ ë¶ˆë¦¿í¬ì¸íŠ¸ë¡œ ì¶”ê°€
                    old_prompt = st.session_state.messages[system_idx]["content"]
                    adjustment = feedback_analysis["suggested_adjustment"]
                    feedback_lines = [line.strip() for line in str(adjustment).splitlines() if line.strip()]
                    feedback_text = "\n" + "\n".join(f"- {line}" for line in feedback_lines)
                    new_system_prompt = old_prompt.rstrip() + feedback_text
                    st.session_state.messages[system_idx]["content"] = new_system_prompt
                    print(f"[SYSTEM PROMPT UPDATED] {new_system_prompt}")  # system prompt ì—…ë°ì´íŠ¸ ë¡œê·¸
                    # ì—…ë°ì´íŠ¸ëœ system promptì™€ ë©”ì‹œì§€ë¡œ assistant ë‹µë³€ ìƒì„±
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=st.session_state.messages,
                        stream=False
                    )
                    full_response = response.choices[0].message.content
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    db.save_message(st.session_state.current_conversation_id, "assistant", full_response)
                    print(f"[LLM RESPONSE] {full_response}")
                st.write("Feedback applied. Please wait for the new response.")
                # st.rerun()
        

        with st.chat_message("assistant"):
            stream_placeholder = st.empty()
            full_response = ""

            try:
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=st.session_state.messages,
                    stream=True
                )

                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°›ê¸°
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        stream_placeholder.markdown(render_with_latex(full_response + "â–Œ"))

                # ìŠ¤íŠ¸ë¦¬ë° ë„ì¤‘ì—ë„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ê³„ì† ê°±ì‹  (ìˆ˜ì‹ í¬í•¨)
                stream_placeholder.empty()
                st.markdown(render_with_latex(full_response))
                # ì‘ë‹µ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
                if not (st.session_state.messages and
                        st.session_state.messages[-1]["role"] == "assistant" and
                        st.session_state.messages[-1]["content"] == full_response):
                    st.session_state.messages.append(
                        {"role": "assistant", "content": full_response}
                    )
                    db.save_message(st.session_state.current_conversation_id, "assistant", full_response)
                
            except Exception as e:
                st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                st.stop()
